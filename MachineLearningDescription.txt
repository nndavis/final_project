We used neural networking for our machine learning model. 
The purpose of our model is to classify whether or not a job is good based on a set of features.
Our original dataset of our joined tables contained 29 columns. During the preprocessing step, we dropped down to 11 columns.
Overall_rating is our target. Our features are; name, job title, industry, location, work life balance, culture values,
diversity inclusion, career opportunity, company benefits, and total employee estimate. We chose these features because they are
most applicable when rating a job. 
Those who are looking for a new job, whether they are unemployed or are looking to switch employers, asks about the features we chose to gauge whether they think the job will be a good fit for them. 
Since we are using neural networking, we had to do some encoding on a few features.
These features included; name, job title, industry, and location. This was done by creating a OneHotEncoder instance, 
and then fit and transform the OneHotEncoder using the categorical variable list. After that we added the encoded variable names
back to the dataframe and dropped the originals.
To make our data viable for our deep learning model we used Scikit-Learn to split the data into training and testing sets.
By default, the split is 80% training and 20% testing.
Scikit-Learn was also used to scale the data. The following lines of code are what we did to split and scale:

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

We chose to use a neural network model because of it's impressive uses when it comes to classification.
A downside to using a neural network is that it is hard to train our data. This may be due to the way our features were encoded.
Another issue is that neural networks have difficulty showing correlation from dependent features to the independent.
The positive to using a neural network is that it uses algorithms that work like a human brain. 
Our data is based off reviews by humans, so it makes sense to use machine learning that mimics those decisions.

The second machine learning model we used is Random Forest Classifier. This is a supervised machine learning algorithm that we are using to classify. Compared to the neural network model, we dropped the overall ratings, id, and good columns. With this model our target is overall rating. Our goal is to calculate the importance of the features. That was calculated by using .feature_importances_ on our model. Our accuracy for this model is 0.98 on both the testing and training data. The precision and recall are very close in values with little to no difference. Overall the model runs well.



#Transfer learning
#SVMs
#XGBoost
